{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIyaUwUdyxUk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "# Method to compute the accuracy. Call predict_labels to get the labels for the dataset\n",
        "def compute_f1(predictions, correct, idx2Label):\n",
        "    label_pred = []\n",
        "    for sentence in predictions:\n",
        "        label_pred.append([idx2Label[element] for element in sentence])\n",
        "\n",
        "    label_correct = []\n",
        "    for sentence in correct:\n",
        "        label_correct.append([idx2Label[element] for element in sentence])\n",
        "\n",
        "    # print(\"predictions \", len(label_pred))\n",
        "    # print(\"correct labels \", len(label_correct))\n",
        "\n",
        "    prec = compute_precision(label_pred, label_correct)\n",
        "    rec = compute_precision(label_correct, label_pred)\n",
        "\n",
        "    f1 = 0\n",
        "    if (rec + prec) > 0:\n",
        "        f1 = 2.0 * prec * rec / (prec + rec);\n",
        "\n",
        "    return prec, rec, f1\n",
        "\n",
        "\n",
        "def compute_precision(guessed_sentences, correct_sentences):\n",
        "    assert (len(guessed_sentences) == len(correct_sentences))\n",
        "    correctCount = 0\n",
        "    count = 0\n",
        "\n",
        "    for sentenceIdx in range(len(guessed_sentences)):\n",
        "        guessed = guessed_sentences[sentenceIdx]\n",
        "        correct = correct_sentences[sentenceIdx]\n",
        "        assert (len(guessed) == len(correct))\n",
        "        idx = 0\n",
        "        while idx < len(guessed):\n",
        "            if guessed[idx][0] == 'B':  # a new chunk starts\n",
        "                count += 1\n",
        "\n",
        "                if guessed[idx] == correct[idx]:  # first prediction correct\n",
        "                    idx += 1\n",
        "                    correctlyFound = True\n",
        "\n",
        "                    while idx < len(guessed) and guessed[idx][0] == 'I':  # scan entire chunk\n",
        "                        if guessed[idx] != correct[idx]:\n",
        "                            correctlyFound = False\n",
        "\n",
        "                        idx += 1\n",
        "\n",
        "                    if idx < len(guessed):\n",
        "                        if correct[idx][0] == 'I':  # chunk in correct was longer\n",
        "                            correctlyFound = False\n",
        "\n",
        "                    if correctlyFound:\n",
        "                        correctCount += 1\n",
        "                else:\n",
        "                    idx += 1\n",
        "            else:\n",
        "                idx += 1\n",
        "\n",
        "    precision = 0\n",
        "    if count > 0:\n",
        "        precision = float(correctCount) / count\n",
        "\n",
        "    return precision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXz73PYuzHRG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "def readfile(filename, *, encoding=\"UTF8\"):\n",
        "    '''\n",
        "    read file\n",
        "    return format :\n",
        "    [ ['EU', 'B-ORG'], ['rejects', 'O'], ['German', 'B-MISC'], ['call', 'O'], ['to', 'O'], ['boycott', 'O'], ['British', 'B-MISC'], ['lamb', 'O'], ['.', 'O'] ]\n",
        "    '''\n",
        "    with open(filename, mode='rt', encoding=encoding) as f:\n",
        "        sentences = []\n",
        "        sentence = []\n",
        "        for line in f:\n",
        "            if len(line) == 0 or line.startswith('-DOCSTART') or line[0] == \"\\n\":\n",
        "                if len(sentence) > 0:\n",
        "                    sentences.append(sentence)\n",
        "                    sentence = []\n",
        "                continue\n",
        "            splits = line.split(' ')\n",
        "            sentence.append([splits[0], splits[-1]])\n",
        "\n",
        "    if len(sentence) > 0:\n",
        "        sentences.append(sentence)\n",
        "        sentence = []\n",
        "    return sentences\n",
        "\n",
        "\n",
        "# define casing s.t. NN can use case information to learn patterns\n",
        "def getCasing(word, caseLookup):\n",
        "    casing = 'other'\n",
        "\n",
        "    numDigits = 0\n",
        "    for char in word:\n",
        "        if char.isdigit():\n",
        "            numDigits += 1\n",
        "\n",
        "    digitFraction = numDigits / float(len(word))\n",
        "\n",
        "    if word.isdigit():  # Is a digit\n",
        "        casing = 'numeric'\n",
        "    elif digitFraction > 0.5:\n",
        "        casing = 'mainly_numeric'\n",
        "    elif word.islower():  # All lower case\n",
        "        casing = 'allLower'\n",
        "    elif word.isupper():  # All upper case\n",
        "        casing = 'allUpper'\n",
        "    elif word[0].isupper():  # is a title, initial char upper, then all lower\n",
        "        casing = 'initialUpper'\n",
        "    elif numDigits > 0:\n",
        "        casing = 'contains_digit'\n",
        "\n",
        "    return caseLookup[casing]\n",
        "\n",
        "\n",
        "# return batches ordered by words in sentence\n",
        "def createEqualBatches(data):\n",
        "\n",
        "\n",
        "    # num_words = []\n",
        "    # for i in data:\n",
        "    #     num_words.append(len(i[0]))\n",
        "    # num_words = set(num_words)\n",
        "\n",
        "    n_batches = 100\n",
        "    batch_size = len(data) // n_batches\n",
        "    num_words = [batch_size*(i+1) for i in range(0, n_batches)]\n",
        "\n",
        "    batches = []\n",
        "    batch_len = []\n",
        "    z = 0\n",
        "    start = 0\n",
        "    for end in num_words:\n",
        "        # print(\"start\", start)\n",
        "        for batch in data[start:end]:\n",
        "            # if len(batch[0]) == i:  # if sentence has i words\n",
        "            batches.append(batch)\n",
        "            z += 1\n",
        "        batch_len.append(z)\n",
        "        start = end\n",
        "\n",
        "    return batches, batch_len\n",
        "\n",
        "def createBatches(data):\n",
        "    l = []\n",
        "    for i in data:\n",
        "        l.append(len(i[0]))\n",
        "    l = set(l)\n",
        "    batches = []\n",
        "    batch_len = []\n",
        "    z = 0\n",
        "    for i in l:\n",
        "        for batch in data:\n",
        "            if len(batch[0]) == i:\n",
        "                batches.append(batch)\n",
        "                z += 1\n",
        "        batch_len.append(z)\n",
        "    return batches,batch_len\n",
        "\n",
        "\n",
        "# returns matrix with 1 entry = list of 4 elements:\n",
        "# word indices, case indices, character indices, label indices\n",
        "def createMatrices(sentences, word2Idx, label2Idx, case2Idx, char2Idx):\n",
        "    unknownIdx = word2Idx['UNKNOWN_TOKEN']\n",
        "    paddingIdx = word2Idx['PADDING_TOKEN']\n",
        "\n",
        "    dataset = []\n",
        "\n",
        "    wordCount = 0\n",
        "    unknownWordCount = 0\n",
        "\n",
        "    for sentence in sentences:\n",
        "        wordIndices = []\n",
        "        caseIndices = []\n",
        "        charIndices = []\n",
        "        labelIndices = []\n",
        "\n",
        "        for word, char, label in sentence:\n",
        "            wordCount += 1\n",
        "            if word in word2Idx:\n",
        "                wordIdx = word2Idx[word]\n",
        "            elif word.lower() in word2Idx:\n",
        "                wordIdx = word2Idx[word.lower()]\n",
        "            else:\n",
        "                wordIdx = unknownIdx\n",
        "                unknownWordCount += 1\n",
        "            charIdx = []\n",
        "            for x in char:\n",
        "                charIdx.append(char2Idx[x])\n",
        "            # Get the label and map to int\n",
        "            wordIndices.append(wordIdx)\n",
        "            caseIndices.append(getCasing(word, case2Idx))\n",
        "            charIndices.append(charIdx)\n",
        "            labelIndices.append(label2Idx[label])\n",
        "\n",
        "        dataset.append([wordIndices, caseIndices, charIndices, labelIndices])\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def iterate_minibatches(dataset, batch_len):\n",
        "    start = 0\n",
        "    for i in batch_len:\n",
        "        tokens = []\n",
        "        caseing = []\n",
        "        char = []\n",
        "        labels = []\n",
        "        data = dataset[start:i]\n",
        "        start = i\n",
        "        for dt in data:\n",
        "            t, c, ch, l = dt\n",
        "            l = np.expand_dims(l, -1)\n",
        "            tokens.append(t)\n",
        "            caseing.append(c)\n",
        "            char.append(ch)\n",
        "            labels.append(l)\n",
        "\n",
        "        yield np.asarray(labels), np.asarray(tokens), np.asarray(caseing), np.asarray(char)\n",
        "\n",
        "\n",
        "# returns data with character information in format\n",
        "# [['EU', ['E', 'U'], 'B-ORG\\n'], ...]\n",
        "def addCharInformation(Sentences):\n",
        "    for i, sentence in enumerate(Sentences):\n",
        "        for j, data in enumerate(sentence):\n",
        "            chars = [c for c in data[0]]\n",
        "            Sentences[i][j] = [data[0], chars, data[1]]\n",
        "    return Sentences\n",
        "\n",
        "\n",
        "# 0-pads all words\n",
        "def padding(Sentences):\n",
        "    maxlen = 52\n",
        "    for sentence in Sentences:\n",
        "        char = sentence[2]\n",
        "        for x in char:\n",
        "            maxlen = max(maxlen, len(x))\n",
        "    for i, sentence in enumerate(Sentences):\n",
        "        Sentences[i][2] = pad_sequences(Sentences[i][2], 52, padding='post')\n",
        "    return Sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfTDstCOxTwI"
      },
      "outputs": [],
      "source": [
        "\"\"\"Load packages\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "#from validation import compute_f1\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import TimeDistributed, Conv1D, Dense, Embedding, Input, Dropout, LSTM, Bidirectional, MaxPooling1D,Flatten, concatenate\n",
        "#from prepro import readfile, createBatches, createMatrices, iterate_minibatches, addCharInformation, padding\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from keras.initializers import RandomUniform\n",
        "from tensorflow.keras.optimizers import SGD, Nadam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFLHfRC9z3DO",
        "outputId": "16e4440c-b44e-4bb8-a72d-92ca0e62f3e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hLtaWNgxTwN",
        "outputId": "a82a7158-dfed-4bfb-a4a7-8b749f1c07f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class initialised.\n"
          ]
        }
      ],
      "source": [
        "\"\"\"Initialise class\"\"\"\n",
        "\n",
        "class CNN_BLSTM(object):\n",
        "\n",
        "    def __init__(self, EPOCHS, DROPOUT, DROPOUT_RECURRENT, LSTM_STATE_SIZE, CONV_SIZE, LEARNING_RATE, OPTIMIZER):\n",
        "\n",
        "        self.epochs = EPOCHS\n",
        "        self.dropout = DROPOUT\n",
        "        self.dropout_recurrent = DROPOUT_RECURRENT\n",
        "        self.lstm_state_size = LSTM_STATE_SIZE\n",
        "        self.conv_size = CONV_SIZE\n",
        "        self.learning_rate = LEARNING_RATE\n",
        "        self.optimizer = OPTIMIZER\n",
        "\n",
        "    def loadData(self):\n",
        "        \"\"\"Load data and add character information\"\"\"\n",
        "        self.trainSentences = readfile(\"/content/drive/My Drive/train.txt\")\n",
        "        self.devSentences = readfile(\"/content/drive/My Drive/valid.txt\")\n",
        "        self.testSentences = readfile(\"/content/drive/My Drive/test.txt\")\n",
        "\n",
        "    def addCharInfo(self):\n",
        "        # format: [['EU', ['E', 'U'], 'B-ORG\\n'], ...]\n",
        "        self.trainSentences = addCharInformation(self.trainSentences)\n",
        "        self.devSentences = addCharInformation(self.devSentences)\n",
        "        self.testSentences = addCharInformation(self.testSentences)\n",
        "\n",
        "    def embed(self):\n",
        "        \"\"\"Create word- and character-level embeddings\"\"\"\n",
        "\n",
        "        labelSet = set()\n",
        "        words = {}\n",
        "\n",
        "        # unique words and labels in data\n",
        "        for dataset in [self.trainSentences, self.devSentences, self.testSentences]:\n",
        "            for sentence in dataset:\n",
        "                for token, char, label in sentence:\n",
        "                    # token ... token, char ... list of chars, label ... BIO labels\n",
        "                    labelSet.add(label)\n",
        "                    words[token.lower()] = True\n",
        "\n",
        "        # mapping for labels\n",
        "        self.label2Idx = {}\n",
        "        for label in labelSet:\n",
        "            self.label2Idx[label] = len(self.label2Idx)\n",
        "\n",
        "        # mapping for token cases\n",
        "        case2Idx = {'numeric': 0, 'allLower': 1, 'allUpper': 2, 'initialUpper': 3, 'other': 4, 'mainly_numeric': 5,\n",
        "                    'contains_digit': 6, 'PADDING_TOKEN': 7}\n",
        "        self.caseEmbeddings = np.identity(len(case2Idx), dtype='float32')  # identity matrix used\n",
        "\n",
        "        # read GLoVE word embeddings\n",
        "        word2Idx = {}\n",
        "        self.wordEmbeddings = []\n",
        "\n",
        "        fEmbeddings = open(\"/content/drive/MyDrive/glove.6B.50d.txt\", encoding=\"utf-8\")\n",
        "\n",
        "        # loop through each word in embeddings\n",
        "        for line in fEmbeddings:\n",
        "            split = line.strip().split(\" \")\n",
        "            word = split[0]  # embedding word entry\n",
        "\n",
        "            if len(word2Idx) == 0:  # add padding+unknown\n",
        "                word2Idx[\"PADDING_TOKEN\"] = len(word2Idx)\n",
        "                vector = np.zeros(len(split) - 1)  # zero vector for 'PADDING' word\n",
        "                self.wordEmbeddings.append(vector)\n",
        "\n",
        "                word2Idx[\"UNKNOWN_TOKEN\"] = len(word2Idx)\n",
        "                vector = np.random.uniform(-0.25, 0.25, len(split) - 1)\n",
        "                self.wordEmbeddings.append(vector)\n",
        "\n",
        "            if split[0].lower() in words:\n",
        "                vector = np.array([float(num) for num in split[1:]])\n",
        "                self.wordEmbeddings.append(vector)  # word embedding vector\n",
        "                word2Idx[split[0]] = len(word2Idx)  # corresponding word dict\n",
        "\n",
        "        self.wordEmbeddings = np.array(self.wordEmbeddings)\n",
        "\n",
        "        # dictionary of all possible characters\n",
        "        self.char2Idx = {\"PADDING\": 0, \"UNKNOWN\": 1}\n",
        "        for c in \" 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ.,-_()[]{}!?:;#'\\\"/\\\\%$`&=*+@^~|<>\":\n",
        "            self.char2Idx[c] = len(self.char2Idx)\n",
        "\n",
        "        # format: [[wordindices], [caseindices], [padded word indices], [label indices]]\n",
        "        self.train_set = padding(createMatrices(self.trainSentences, word2Idx, self.label2Idx, case2Idx, self.char2Idx))\n",
        "        self.dev_set = padding(createMatrices(self.devSentences, word2Idx, self.label2Idx, case2Idx, self.char2Idx))\n",
        "        self.test_set = padding(createMatrices(self.testSentences, word2Idx, self.label2Idx, case2Idx, self.char2Idx))\n",
        "\n",
        "        self.idx2Label = {v: k for k, v in self.label2Idx.items()}\n",
        "\n",
        "    def createBatches(self):\n",
        "        \"\"\"Create batches\"\"\"\n",
        "        self.train_batch, self.train_batch_len = createBatches(self.train_set)\n",
        "        self.dev_batch, self.dev_batch_len = createBatches(self.dev_set)\n",
        "        self.test_batch, self.test_batch_len = createBatches(self.test_set)\n",
        "\n",
        "    def tag_dataset(self, dataset, model):\n",
        "        \"\"\"Tag data with numerical values\"\"\"\n",
        "        correctLabels = []\n",
        "        predLabels = []\n",
        "        for i, data in enumerate(dataset):\n",
        "            tokens, casing, char, labels = data\n",
        "            tokens = np.asarray([tokens])\n",
        "            casing = np.asarray([casing])\n",
        "            char = np.asarray([char])\n",
        "            pred = model.predict([tokens, casing, char], verbose=False)[0]\n",
        "            pred = pred.argmax(axis=-1)  # Predict the classes\n",
        "            correctLabels.append(labels)\n",
        "            predLabels.append(pred)\n",
        "        return predLabels, correctLabels\n",
        "\n",
        "    def buildModel(self):\n",
        "        \"\"\"Model layers\"\"\"\n",
        "\n",
        "        # character input\n",
        "        character_input = Input(shape=(None, 52,), name=\"Character_input\")\n",
        "        embed_char_out = TimeDistributed(\n",
        "            Embedding(len(self.char2Idx), 30, embeddings_initializer=RandomUniform(minval=-0.5, maxval=0.5)), name=\"Character_embedding\")(\n",
        "            character_input)\n",
        "\n",
        "        dropout = Dropout(self.dropout)(embed_char_out)\n",
        "\n",
        "        # CNN\n",
        "        conv1d_out = TimeDistributed(Conv1D(kernel_size=self.conv_size, filters=30, padding='same', activation='tanh', strides=1), name=\"Convolution\")(dropout)\n",
        "        maxpool_out = TimeDistributed(MaxPooling1D(52), name=\"Maxpool\")(conv1d_out)\n",
        "        char = TimeDistributed(Flatten(), name=\"Flatten\")(maxpool_out)\n",
        "        char = Dropout(self.dropout)(char)\n",
        "\n",
        "        # word-level input\n",
        "        words_input = Input(shape=(None,), dtype='int32', name='words_input')\n",
        "        words = Embedding(input_dim=self.wordEmbeddings.shape[0], output_dim=self.wordEmbeddings.shape[1], weights=[self.wordEmbeddings],\n",
        "                          trainable=False)(words_input)\n",
        "\n",
        "        # case-info input\n",
        "        casing_input = Input(shape=(None,), dtype='int32', name='casing_input')\n",
        "        casing = Embedding(output_dim=self.caseEmbeddings.shape[1], input_dim=self.caseEmbeddings.shape[0], weights=[self.caseEmbeddings],\n",
        "                           trainable=False)(casing_input)\n",
        "\n",
        "        # concat & BLSTM\n",
        "        output = concatenate([words, casing, char])\n",
        "        output = Bidirectional(LSTM(self.lstm_state_size,\n",
        "                                    return_sequences=True,\n",
        "                                    dropout=self.dropout,                        # on input to each LSTM block\n",
        "                                    recurrent_dropout=self.dropout_recurrent     # on recurrent input signal\n",
        "                                   ), name=\"BLSTM\")(output)\n",
        "        output = TimeDistributed(Dense(len(self.label2Idx), activation='softmax'),name=\"Softmax_layer\")(output)\n",
        "\n",
        "        # set up model\n",
        "        self.model = Model(inputs=[words_input, casing_input, character_input], outputs=[output])\n",
        "\n",
        "        self.model.compile(loss='sparse_categorical_crossentropy', optimizer=self.optimizer)\n",
        "\n",
        "        self.init_weights = self.model.get_weights()\n",
        "\n",
        "        plot_model(self.model, to_file='model.png')\n",
        "\n",
        "        print(\"Model built. Saved model.png\\n\")\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Default training\"\"\"\n",
        "\n",
        "        self.f1_test_history = []\n",
        "        self.f1_dev_history = []\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            print(\"Epoch {}/{}\".format(epoch, self.epochs))\n",
        "            for i,batch in enumerate(iterate_minibatches(self.train_batch,self.train_batch_len)):\n",
        "                labels, tokens, casing,char = batch\n",
        "                self.model.train_on_batch([tokens, casing,char], labels)\n",
        "\n",
        "            # compute F1 scores\n",
        "            predLabels, correctLabels = self.tag_dataset(self.test_batch, self.model)\n",
        "            pre_test, rec_test, f1_test = compute_f1(predLabels, correctLabels, self.idx2Label)\n",
        "            self.f1_test_history.append(f1_test)\n",
        "            print(\"f1 test \", round(f1_test, 4))\n",
        "\n",
        "            predLabels, correctLabels = self.tag_dataset(self.dev_batch, self.model)\n",
        "            pre_dev, rec_dev, f1_dev = compute_f1(predLabels, correctLabels, self.idx2Label)\n",
        "            self.f1_dev_history.append(f1_dev)\n",
        "            print(\"f1 dev \", round(f1_dev, 4), \"\\n\")\n",
        "\n",
        "        print(\"Final F1 test score: \", f1_test)\n",
        "\n",
        "        print(\"Training finished.\")\n",
        "\n",
        "        # save model\n",
        "        self.modelName = \"{}_{}_{}_{}_{}_{}_{}\".format(self.epochs,\n",
        "                                                        self.dropout,\n",
        "                                                        self.dropout_recurrent,\n",
        "                                                        self.lstm_state_size,\n",
        "                                                        self.conv_size,\n",
        "                                                        self.learning_rate,\n",
        "                                                        self.optimizer.__class__.__name__\n",
        "                                                       )\n",
        "\n",
        "        modelName = self.modelName + \".h5\"\n",
        "        self.model.save(modelName)\n",
        "        print(\"Model weights saved.\")\n",
        "\n",
        "        self.model.set_weights(self.init_weights)  # clear model\n",
        "        print(\"Model weights cleared.\")\n",
        "\n",
        "    def writeToFile(self):\n",
        "        \"\"\"Write output to file\"\"\"\n",
        "\n",
        "        # .txt file format\n",
        "        # [epoch  ]\n",
        "        # [f1_test]\n",
        "        # [f1_dev ]\n",
        "\n",
        "        output = np.matrix([[int(i) for i in range(self.epochs)], self.f1_test_history, self.f1_dev_history])\n",
        "\n",
        "        fileName = self.modelName + \".txt\"\n",
        "        with open(fileName,'wb') as f:\n",
        "            for line in output:\n",
        "                np.savetxt(f, line, fmt='%.5f')\n",
        "\n",
        "        print(\"Model performance written to file.\")\n",
        "\n",
        "    print(\"Class initialised.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8bYKeyXxTwR"
      },
      "outputs": [],
      "source": [
        "\"\"\"Set parameters\"\"\"\n",
        "\n",
        "EPOCHS = 30               # paper: 80\n",
        "DROPOUT = 0.5             # paper: 0.68\n",
        "DROPOUT_RECURRENT = 0.25  # not specified in paper, 0.25 recommended\n",
        "LSTM_STATE_SIZE = 200     # paper: 275\n",
        "CONV_SIZE = 3             # paper: 3\n",
        "LEARNING_RATE = 0.0105    # paper 0.0105\n",
        "OPTIMIZER = Nadam()       # paper uses SGD(lr=self.learning_rate), Nadam() recommended\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "C52t8LyrxTwS",
        "scrolled": true,
        "outputId": "3595b8d0-1dca-4019-8559-e945fbc7a419"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model built. Saved model.png\n",
            "\n",
            "Epoch 0/30\n",
            "f1 test  0.151\n",
            "f1 dev  0.1706 \n",
            "\n",
            "Epoch 1/30\n",
            "f1 test  0.4526\n",
            "f1 dev  0.4943 \n",
            "\n",
            "Epoch 2/30\n",
            "f1 test  0.6068\n",
            "f1 dev  0.6576 \n",
            "\n",
            "Epoch 3/30\n",
            "f1 test  0.6731\n",
            "f1 dev  0.7068 \n",
            "\n",
            "Epoch 4/30\n",
            "f1 test  0.7089\n",
            "f1 dev  0.7383 \n",
            "\n",
            "Epoch 5/30\n",
            "f1 test  0.7397\n",
            "f1 dev  0.7534 \n",
            "\n",
            "Epoch 6/30\n",
            "f1 test  0.7384\n",
            "f1 dev  0.7363 \n",
            "\n",
            "Epoch 7/30\n",
            "f1 test  0.773\n",
            "f1 dev  0.7843 \n",
            "\n",
            "Epoch 8/30\n",
            "f1 test  0.7763\n",
            "f1 dev  0.7983 \n",
            "\n",
            "Epoch 9/30\n",
            "f1 test  0.7906\n",
            "f1 dev  0.8104 \n",
            "\n",
            "Epoch 10/30\n",
            "f1 test  0.7806\n",
            "f1 dev  0.8047 \n",
            "\n",
            "Epoch 11/30\n",
            "f1 test  0.7885\n",
            "f1 dev  0.8065 \n",
            "\n",
            "Epoch 12/30\n",
            "f1 test  0.7797\n",
            "f1 dev  0.8082 \n",
            "\n",
            "Epoch 13/30\n",
            "f1 test  0.8038\n",
            "f1 dev  0.8248 \n",
            "\n",
            "Epoch 14/30\n"
          ]
        }
      ],
      "source": [
        "\"\"\"Construct and run model\"\"\"\n",
        "\n",
        "cnn_blstm = CNN_BLSTM(EPOCHS, DROPOUT, DROPOUT_RECURRENT, LSTM_STATE_SIZE, CONV_SIZE, LEARNING_RATE, OPTIMIZER)\n",
        "cnn_blstm.loadData()\n",
        "cnn_blstm.addCharInfo()\n",
        "cnn_blstm.embed()\n",
        "cnn_blstm.createBatches()\n",
        "cnn_blstm.buildModel()\n",
        "cnn_blstm.train()\n",
        "cnn_blstm.writeToFile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMETCnN45KwL"
      },
      "source": [
        "# Nouvelle section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xv0fldXrxTwT"
      },
      "source": [
        "# Plot learning curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aUIsDkgxTwV"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xg6GTQ0xTwW",
        "outputId": "b66cf537-9a14-4b3c-ecd9-e8537008eea4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-14ccff9302e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn_blstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf1_test_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"F1 test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn_blstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf1_dev_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"F1 dev\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epochs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"F1 score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'cnn_blstm' is not defined"
          ]
        }
      ],
      "source": [
        "plt.plot(cnn_blstm.f1_test_history, label = \"F1 test\")\n",
        "plt.plot(cnn_blstm.f1_dev_history, label = \"F1 dev\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"F1 score\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qx0Rg3H3xTwX"
      },
      "source": [
        "# Label distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9NEsNNhxTwY",
        "outputId": "b5d9771b-0252-4fed-bca9-29d28a9a6df2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "B-ORG: 3.1%\n",
            "I-ORG: 1.82%\n",
            "B-MISC: 1.69%\n",
            "I-MISC: 0.57%\n",
            "B-LOC: 3.51%\n",
            "I-LOC: 0.57%\n",
            "B-PER: 3.24%\n",
            "I-PER: 2.22%\n",
            "O: 83.28%\n"
          ]
        }
      ],
      "source": [
        "cnn_blstm = CNN_BLSTM(EPOCHS, DROPOUT, DROPOUT_RECURRENT, LSTM_STATE_SIZE, CONV_SIZE, LEARNING_RATE, OPTIMIZER)\n",
        "cnn_blstm.loadData()\n",
        "\n",
        "category_count = {\"B-ORG\\n\": 0, \"I-ORG\\n\":0, \"B-MISC\\n\": 0, \"I-MISC\\n\":0, \"B-LOC\\n\": 0, \"I-LOC\\n\": 0, \"B-PER\\n\": 0, \"I-PER\\n\": 0, \"O\\n\": 0}\n",
        "total_count = 0\n",
        "\n",
        "for sentence in cnn_blstm.trainSentences:\n",
        "    for word in sentence:\n",
        "        if word[1] in category_count.keys():\n",
        "            category_count[word[1]] += 1\n",
        "            total_count += 1\n",
        "\n",
        "for category, count in category_count.items():\n",
        "    print(\"{}: {}%\".format(category.replace(\"\\n\", \"\"), round((count/total_count)*100, 2)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3NE6ysaxTwY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}